
## Notes - eBook - SQL Execution plan - Grant Fritchey (3rd edition).pdf ##

—------------------------------------------------------------------------

Chapter 1: Introducing the Execution Plan
Chapter 2: Getting Started Reading Plans
Chapter 3: Data Reading Operators
Chapter 4: Joining Data
Chapter 5: Sorting and Aggregating Data
Chapter 6: Execution Plans for Data Modifications
Chapter 7: Execution Plans for Common T-SQL Statements
Chapter 8: Examining Index Usage
Chapter 9: Exploring Plan Reuse
Chapter 10: Controlling Execution Plans with Hints
Chapter 11: Parallelism in Execution Plans
Chapter 12: Batch Mode Processing
Chapter 13: The XML of Execution Plans
Chapter 14: Plans for Special Data Types and Cursors
Chapter 15: Automating Plan Capture
Chapter 16: The Query Store
Chapter 17: SSMS Tools for Exploring Execution Plans



## ------------------------------------- Chapter 1: Introducing the Execution Plan -------------------------------------##


## What is the Execution plan?

- Is a set of instructions devised by SQL server Optimizer to perform the result required by the end user.
- Execution plan will reveal what tables, indexes,how much data is accessed, order of execution, any aggregation used

### What Happens When a Query is Submitted?

- **Firstly**, when a query is fired, the query undergoes **PARSING** (i.e. checking semantics, syntax of the query) & Query binding phase - where objects mentioned in the query exist are verified.
- **Secondly**, the query undergoes query **COMPILATION** - i.e generates an execution plan that has a series of instructions for processing the query.
- Usually the query plan is stored in the Plan cache (part of the memory), if a similar query is used then the same cached plan is sent to the execution engine.
- **Finally**, Query execution engine executes the instructions laid out in plan & generates the call to the storage engine to retrieve data required.

    
### **QUERY COMPILATION PHASE**
—-------------------------------
The following sections review briefly what happens during query compilation,

1. Parsing Phase
2. Binding Phase
3. Optimization phase

**PARSING PHASE**
………………

- When a user request query reaches SQL Server, the query will undergo a process called PARSING.
- Here, validation happens to check that the T-SQL is written correctly.
- **For example**, if you type SELETC instead of SELECT, then parsing stops and SQL Server returns an error to the query source.
- The output of the Parser process is a **PARSE TREE**, or **QUERY TREE** (or it's even called a **SEQUENCE TREE**). The parse tree represents the logical steps necessary to execute the requested query.

**BINDING PHASE (validation of objects in query)**
…………………………………………………..

- If the T-SQL string has parsed correctly, the parse tree passes to the **Algebrizer**, which performs a process called query **BINDING**.
- The Algebrizer checks to resolve all the names of the various objects, tables, and columns referred to within the query string. It identifies, at the individual column level, all the data types (varchar(50) versus datetime and so on) for the objects being accessed.
- It also determines the location of **Aggregates**, such as SUM and MAX, within the query, a process called aggregate binding.
- The output of the Algebrizer is called the **QUERY PROCESSOR TREE**, which is then passed on to the query optimizer. The output also includes a **HASH**, a coded value representing the query.
- In the Query processing phase, the HASH is used to check if there is already an existing plan for the same query, if YES, it stops and Reuses the same plan from Plan Cache.

***Note***:

A plan is considered invalid when there is any
- changes done to table (such as ADDING or DROPPING INDEXES)
OR
- When the STATISTICS were refreshed since the plan was created or stored.

**OPTIMIZATION PHASE**
……………………….
The query optimizer is a piece of software that considers many alternate ways to achieve the best / suitable query result.
Here the optimizer will generate many plans with the best plan in terms of COST within as little time as is reasonable.
Most queries submitted to SQL Server will be subject to either
- **Cost-based plan**: uses Full cost-based optimization.
- **Trivial plan**: Short cut plan for less critical / SIMPLE queries.

**Full cost-based optimization**

The full cost-based optimization process takes three inputs:

1. **Query processor tree**:
- gives the optimizer knowledge of the logical structure of the objects i.e. tables and indexes used in the query.
1. **Statistics**:
- Understanding of the Volume of data distributed with indexes, columns etc
1. **Constraints**:
- Any business logic included like primary key, foreign key, default values and any other conditions.

**Using these inputs, the optimizer creates a model**, essentially a set of rules
required to transform the logical query tree into a plan containing a **set of**
**operators** that, collectively, will physically execute the query.

- Each operator performs a dedicated task
- The optimizer will generate and evaluate many possible plans with different different types of Joins / Order / indexes
- **Generally, the optimizer will choose the plan** that has best plan in terms of
**lowest** total cost, in terms of the sum of the estimated **CPU and I/O processing**
**costs.**

**Trivial plans**

- For very simple queries, the optimizer uses a trivial plan, rather than go through the full cost-based optimization process.
- The optimizer's rules for deciding when it can simply use a trivial plan are unclear, and probably complex. However, for example, a very simple query, such as a SELECT statement against a single table with no aggregates or calculations

**For example**:

use AdventureWorks2019
go
SELECT d.Name FROM HumanResources.Department AS d WHERE
d.DepartmentID = 5

Note:

- Some DDL tasks such CREATE DATABASE, CREATE INDEX etc DO NOT Have many ways to create database objects so these processes DO NOT undergo Optimization.
- Other DDL processes such as ALTER INDEX, ALTER TABLE can undergo optimization processes by optimizer.

- **QUERY EXECUTION PHASE**
   ……………………………………………………….
    
- The query execution engine executes the query per the instructions set out in the execution plan.
- At runtime, the execution engine cannot change the optimizer's plan. However, it can under certain circumstances force a plan to be recompiled.

***Introduced in SQL Server 2017***, there is also the possibility of ***interleaved execution*** when the
object being referenced in the query is a multi-statement table valued user-defined function. During an interleaved execution, the optimizer generates a plan for the query,
In the usual fashion, then the optimization phase pauses, the pertinent subtree of a given plan is executed to get the actual row counts, and the optimizer then uses the actual row counts to optimize the remainder of the query.
**The importance of statistics**

- As we've discussed, the optimizer will choose the lowest-cost plan, based on estimated cost. These estimates are the statistics on your indexes and data. So, the quality of the plan depends on the quality of the statistics (***Best***: Latest stats, ***bad***: *Outdated stats*)
- Optimizer relies on statistics, aggregated information based on a sample of the data.
- For enhancing the statistics, there is New cardinality estimator in SQL Server 2014.
- The estimated cost of an execution plan depends largely on its ***CARDINALITY ESTIMATIONS.***
- ***Cardinality estimation (CE*)** is how the Query Optimizer can estimate the total number of rows processed at each level of a query plan.
- These cardinality estimations rely on statistics of **data distribution**, i.e. the number of different values present, and how many occurrences of each value. This in turn determines the **SELECTIVITY** of the data. **A column such as "gender," for example**, will likely have a **low selectivity**.
- If statistics don't exist for any column that is referenced in the query plan then, by default, they'll be created immediately, in order for the optimizer to consume them.
- The information that makes up **statistics is divided into three subsections**:
    1. **Header** – general data about a given set of statistics.
    2. **Density graph** – the selectivity, uniqueness, of the data, and,
    3. **Histogram** – a tabulation of counts of the occurrence of a particular value, taken from up to 200 data points that are chosen to best represent the complete data in the table.
- There are other statistical considerations too, For example, **table variables *do not ever have statistics*** generated on them, so the optimizer makes assumptions about them, regardless of their actual size.
    1. ***Prior to SQL Server 2014***, that assumption was for one row.
    2. ***From SQL Server 2014 and SQL Server 2016*** now assume one hundred rows in multi-statement user-defined functions, but remain with the one row for all other objects.
    3. ***from SQL Server 2017*** can, in some instances, use interleaved execution to arrive at more accurate row counts for these functions.
- **Temporary tables do have statistics** generated on them and **their statistics are stored in the same type of histogram as permanent tables**, and the optimizer can make use of these statistics.
- 

**The PLAN CACHE and PLAN REUSE**
—----------------------------------

SQL Server will store plans in a section of memory called the plan cache, and reuse those plans wherever possible, to reduce that overhead.

There are a few reasons why the plan for a previously executed query may no longer be in the cache.

1. Plan aged out of the cache.
2. Forced out due to memory pressure
3. someone manually clearing the cache.
4. certain changes to the underlying database schema, or statistics.

**PLAN AGING**
—--------------
- Each plan has an associated "age" value that is the estimated CPU cost of compiling the plan multiplied by the number of times it has been used.
- So, for example, a plan with an estimated compilation cost of 10 that has been referenced 5 times has an "age" value of 50.
- Frequently-referenced plans that are expensive to compile will remain in the cache for as long as possible.
- For example, if the system is under memory pressure, plans may be aged, and cleared out, more aggressively. Also, plans with the lowest age value can be forced out of the cache if the cache is full and memory is required to store newer plans.
- This can become a problem if the optimizer is being forced to produce a very high volume of plans, many of which are only ever used one time by one query, constantly forcing older plans to be flushed from the cache. This is a problem known as **cache churn**.

**Manually clearing the plan cache**
- Sometimes, for testing, you may want to flush all plans from the cache, to see how long a plan takes to compile, or to investigate how minor query adjustments might lead to slightly different plans.
- The command **DBCC FREEPROCCACHE** will clear the cache for all databases on the server. In a production environment, that can result in a significant and sustained performance hit because then each subsequent query is a "new" query and must go through the optimization process.
- We can flush only specific queries or plans by supplying a **plan_handle** or **sql_handle**. You can retrieve these values from either the plan cache itself using Dynamic Management Views (DMVs) such as **sys.dm_exec_query_stats**
- **Once you have the value, simply run DBCC FREEPROCCACHE() to remove a specific plan from the plan cache.**
- Similarly, we can use DBCC FLUSHPROCINDB(db_id) to remove all plans for a specific database, but the command is not officially documented.
- **SQL Server 2016** introduced a **new, fully-documented** method to remove all plans for a single database, which is to run the following command within the target database:
    - ALTER DATABASE SCOPED CONFIGURATION CLEAR PROCEDURE_CACHE Criteria for plan reuse
- **Example**
    - **– https://stackoverflow.com/questions/52894839/alter-database-scoped-configuration-clear-procedure-cache-not-removing-cache**
    - - create test procedure
                create or alter procedure pTest1
                as begin
                select * from salesLT.product
                where ProductCategoryID =27
                end
                go
        
        - - exec the procedure once.
                exec pTest1
                - - check for cached plan for this specific procedure - cached plan exists
        
        select * from sys.dm_exec_cached_plans p cross apply sys.dm_exec_sql_text(p.plan_handle) st where p.objtype = 'proc' and st.objectid = OBJECT_ID('pTest1')
        - - clear plan cache ALTER DATABASE SCOPED CONFIGURATION CLEAR PROCEDURE_CACHE;
        - - check for cached plan for this specific procedure - not exists anymore
        
        select * from sys.dm_exec_cached_plans p cross apply sys.dm_exec_sql_text(p.plan_handle) st where p.objtype = 'proc' and st.objectid = OBJECT_ID('pTest1')
        
        - - cleanup
               drop procedure pTest1
               select 'cleanup complete'
        
      **Avoiding cache churn: query parameterization**
      —-----------------------------------------------------
        - If we submit ad hoc queries to SQL Server and use hard-coded literal values then, for most of those queries, SQL Server will be forced to complete the full optimization process and compile a new plan each time.
        - On a busy server, this can quickly lead to cache bloat, and to older plans being forced relatively quickly from the cache.
        
        – For example,
        
        – let's say we submit the query in Listing 1-2.
        
        SELECT p.ProductID , p.Name AS ProductName ,
                pi.Shelf , l.Name AS LocationName FROM Production.Product p
                INNER JOIN Production.ProductInventory AS pi ON pi.ProductID = p.ProductID
                INNER JOIN Production.Location AS l ON l.LocationID = pi.LocationID WHERE l.Name = 'Paint';
        
        - We then submit the same query again, but for a different location name (say, 'Tool Cribs' instead of 'Paint'). This will result in two separate plans stored in cache, even though the two queries are essentially the same (they will have the same QueryHash values, assuming no other changes are made).
        - To ensure plan reuse, it's best to use either stored procedures or parameterized queries, where the variables within the query are identified with parameters, rather than hard-coded literals, and we simply pass in the required parameter values at runtime.
        - Another way to mitigate the churn from ad hoc queries is to use a server setting called **Optimize For Ad Hoc Workloads**.
        - Turning this on will cause the optimizer to create what is known as a "**plan stub**" in the plan cache, instead of putting the entire plan there the first time a plan is created. This means that single-use plans will take up radically less memory in your plan cache.
        
        **Plan recompilation**
        —----------------------
        
        - Certain events and actions, such as changes to an index used by a query, can cause a plan to be recompiled, which simply means that the existing plan will be marked for recompilation, and a new plan generated the next time the query is called.
        - Recompiling execution plans can be a very expensive operation.
        - The following actions can lead to recompilation of an execution plan
        - changing the structure of a table, view or function referenced by the query
        - changing, or dropping, an index used by the query
        - Updating the statistics used by the query
        - calling the function sp_recompile
        - mixing DDL and DML within a single batch
        - changing certain SET options within the T-SQL of the batch
        - changes to cursor options within the query
        - deferred compiles • changes to a remote rowset if you're using a function like
        
        OPENQUERY.
            
    **Getting Started with Execution Plans**
        —-----------------------------------------
        Permissions required to view execution plans
        - **GRANT SHOWPLAN TO [username]**;
        Additionally, in order to run the queries against the Dynamic Management Objects (DMO), either
        - **VIEW SERVER STATE** or **VIEW DATABASE STATE**
        **Execution plan formats**
        —--------------------------
    
    SQL Server can output the execution plan in three different ways:
    
    - as an XML plan
    - as a text plan
    - as a graphical plan
    
    **XML plans**
        —----------
        We can use one of the following two commands to retrieve the plan in XML format: • **SET SHOWPLAN_XML ON** – generates the estimated plan (i.e. the query is not executed).
        - **SET STATISTICS_XML ON** – generates the actual execution plan (i.e. with runtime information).
    
    **Text plans**
        —-----------    
    quite difficult to read, but detailed information is immediately available.
    Text plans are on the deprecation list from Microsoft. They will not be available in a future version of SQL Server.
    we can use to retrieve the plan in text format:
    
    - **SET SHOWPLAN_ALL ON** – retrieves the estimated execution plan for the query.
    - **SET STATISTICS PROFILE ON** – retrieves the actual execution plan for the query.
    - **SET SHOWPLAN_TEXT ON** – retrieves the estimated plan but with a very limited set of data, for use with tools like osql.exe.
    

    **Graphical plans**
        —-----------------
        They are quick and easy to read
        **Retrieving cached plans**
        —-------------------------
        The salient point is that the query optimizer produces the plan, and there is only one valid execution plan for a query, at any given time.
        The components of a graphical execution plan
        —---------------------------------------------
    
    Operators
        The operators in a plan tell us exactly how SQL Server chose to execute a query, such as how it chose to access the data in a certain table, 
        how it chose to join that data to rows in a second table, how and where it chose to perform any aggregations, sorting, calculations, and so on.



    ## -------------------------------- Chapter 2: Getting Started Reading Plans:-------------------------------------- ##




    A graphical execution plan displays 3 types of operators
    
    - **Physical Operators** (and their associated logical operations)
    - **Cursor Operators**
    - **Language Elements**
    
    **Physical Operators (and their associated logical operations):**
    - appears in the BLUE icon, representing query execution.
    - They include DML and parallelism operators.
    
    **Cursor Operators**
     - have yellow icons and represent T-SQL cursor operations.
     **Language Elements:**
     - are green icons and represent T-SQL language elements, such
        as
        ASSIGN,
        DECLARE,
        IF,
        WHILE, and so on.
    

The following table lists some of the more common physical operators, categorized according to their basic purpose.
![image.png](attachment:7c6c3f0e-80b4-4ccb-aa2a-e131229a4882:image.png)

![image.png](attachment:58afe766-4c03-47ec-acdc-245b1eb0b36a:image.png)

- for deep knowledge of operator internals, then I recommend Paul White's blog (http://preview.tinyurl.com/y75n6f5z)
- Each operator has a different set of characteristics.
    - For example, Primarily **Sort, Hash Match, and Adaptive Join**, require a variable amount of memory in order to execute.

**Reading a plan: right to left, or left to right?**

—-------------------------------------------------

- Generally read execution plans from right to left, following the data flow arrows.
- But it is equally valid, and frequently helpful, to read from left to right.

SELECT TOP ( 5 )
BusinessEntityID ,
PersonType ,
NameStyle ,
Title ,
FirstName ,
LastName , ModifiedDate
FROM Person.Person
WHERE ModifiedDate >= '20130601'
AND ModifiedDate <= CURRENT_TIMESTAMP ;

If we read the plan from right to left, following the data flow direction, the first action in the plan is to read the data from the Person table, via a Clustered Index Scan. The data passes to the Top operator, which in turn passes the first five rows back to the SELECT.

**Streaming versus blocking operators**
—-----------------------------------------

- **A streaming operator** creates output data at the same time as it receives the input.
    - Example: Clustered Index Scan
- **A Blocking operator** must gather the entire set of input data and then perform some work on the entire data set, before passing on any rows to the next level.
    - In this sort of situation, especially for a very large input, such blocking operators could slow down performance.
    - **For example**: ORDER operator, add ORDER BY ModifiedDate to above query & notice the execution plan.
        
        SELECT TOP ( 5 ) BusinessEntityID , PersonType , NameStyle ,  Title ,FirstName ,  LastName        
                    , ModifiedDate 
                FROM Person.Person
                WHERE ModifiedDate >= '20130601' AND ModifiedDate <= CURRENT_TIMESTAMP
                ORDER BY ModifiedDate
               
- **Semi-blocking**: this operator must complete only part of their work before releasing the first row.

**For example**,
Join operator **Hash Match** first processes all rows from its first input, but then processes and returns rows from the second input as it reads them.

**What to Look for in an Execution Plan**
—---------------------------------------
Topics
..........
> First operator
> Warnings
> Estimated versus actual number of rows
> Operator cost
> Data flow
> Extra operators
> Read operators



First operator
……………………………………

    - The first operator, on the left-hand side of the execution plan, is the **SELECT/INSERT/ UPDATE/DELETE** (and sometimes others, such as MERGE) operator, and the first time you look at an execution plan it's always worth examining its properties.
    - The first operator offers a lot of information about the plan itself and its generation.
    - It has information on
        - time,
        - CPU and
        - Memory required to compile the plan,
        - ANSI connection settings,
        - whether the optimizer completed optimization or terminated the optimization process early.
    - sometimes, it shows WARNING sysmbol in YELLOW color.

Warnings 
…………………………

    - Within an execution plan, you may see (on SQL Server 2012 and later) small icons appear on an operator, specifically a yellow or red exclamation mark. These are warnings. 
    - Not every warning indicates a grave problem, but whenever you see one, check the properties for that icon, which will contain a description of the warning.


Estimated versus actual number of rows
—----------------------------------------------

    - All costs you will see in a plan are based on cardinality estimations, Not dependent on actual row and execution counts.
    - Before diving into analyzing execution plan, it is best practice to check the ACTUAL ROW counts and ESTIMATED ROW counts and make sure difference is not huge.
    - Sometimes, you'll see an operator with a very high estimated cost, because the optimizer estimated it would need to process many rows, when in fact it had to process very few rows (or vice versa, for low estimated costs).
    
Operator cost
—--------------

    - Once we verify cardinality estimates are accurate, we can look for the costliest operators as a means of determining where to focus our initial efforts.
    - It's often useful to compare the cost of one operator to another within the plan. However, we can't compare operator cost within one plan to operator cost within a second plan because the cost estimates are mathematical constructs and don't really lend themselves directly to that type of comparison.
    - Some operators like  Compute Scalar, don't have costs associated with them, or they're "fixed" costs based on assumptions by optimizer, which may or may not be accurate. 
        For example, 
           -- a Compute Scalar operator always has a very low fixed cost (zero-point-lots-of-zeros-one), which is often fine but occasionally misleading.
    - While cost estimates are important and we will use them, just remember that they can't be blindly trusted as an accurate measure of actual cost within the plan.

Data flow
—------------

    - The data flow represents the flow of data, are frequently referred to as pipes. The thickness of the pipe is based on actual row count when available (actual execution plan), and on estimates otherwise (cached or estimated plan).
    - A thicker pipe indicates more data being processed; a thinner pipe indicates less data.  
        For example: 
            -- a very fat pipe at the beginning of a plan narrowing to a very thin pipe on the left-hand side of the plan suggests that filtering is happening late. Small pipes that get bigger and bigger suggest that your query is somehow multiplying data.

Extra operators
—------------------

    - an operator that you've seen and understand, but can't determine why it's in the spot it's in within the plan, then that is an "extra" operator. It's an operator that you don't know, or you don't understand why it's affecting the plan.

Read operators
—----------------
    - scan and the seek read operators.
    - A scan operator (an Index Scan or Table Scan) is just an indicator that reads across the pages in an index or a table.
    - A seek operator is an indicator that uses the structure of an index to find a starting point, and ending point, for a targeted scan through the pages of an index. 
    - A seek indicates, most of the time, that only a small number of rows are being accessed.


The Information Behind the First Operator
—----------------------------------------------

    A lot of valuable information about the plan, as a whole will be in the first operator.
    That's why the first operator in a plan, reading left to right, makes a good starting point for exploring the execution plan of any query. 
    The official name of the first operator is the Result Showplan operator.

Properties of operators
—--------------------------

    Cached plan size – indicates how much memory the plan  is used within the plan cache of SQL Server.
    CardinalityEstimationModelVersion – Starting with SQL Server 2014, a new cardinality estimator is used by the optimizer. You can tell if the plan in question is using the new or the old. The value in the new estimator. If it was 70, it would be the old version from SQL Server 7.
    CompileCPU, CompileMemory, CompileTime – The resources used to by optimizer to produce the plan. The time is in milliseconds. The memory is in kilobytes.
    RetrievedFromCache – Plan was pulled from cache, if value is "False" then plan in is not stored in cache.
    QueryTimeStats – Introduced in SQL Server 2016, this property tells the execution time for the query, when you're capturing an actual query.
    

Optimization level 
—---------------------

    The kind of optimization required to produce the plan. Generally, you'll see either "Trivial" or "Full."
    A Trivial plan for query can be executed only by one way due to lack of choices available for optimizer.
    For example 
    SELECT * statement against a single table without a WHERE clause can only be resolved one way. 
    INSERT statement against a table using VALUES. This can only be resolved a single way by the optimizer, making the plan trivial.
    
    A Full optimization Plan  just means the optimizer has many possibilities of getting the best plan in terms of low cost for executing a query.
    For example.
            To see the optimization level in action, we'll add a JOIN to the query
    We also see a value for a property called Reason For Early Termination Of Statement Optimization. If a plan is produced via the FULL optimization process, then there will be a reason for the optimizer to stop processing and present its selected plan.
    For simple queries, the reason you'll commonly see here is Good Enough Plan Found. This means that after at least one of the optimization phases, the estimated cost of the cheapest plan was below the threshold for entering the next phase, and therefore the optimizer selected that plan as good enough
    Timeout and Memory Limit Exceeded. 
    A value of Timeout indicates that the optimizer attempted to go through its full optimization process, but didn't succeed. Instead, it ran through as many optimization attempts, but it didn't find what it considered to be a mathematically good enough plan. So, it returned the least-cost plan that it had found so far. 
    A value of Memory Limit Exceeded means an extremely large and complex query against very complex structures. The plan generated is probably not optimal for the query if you have a Timeout or Memory Limit Exceeded.

Parameter List
—-----------------
    
    For the below query that has a hard-coded value for column with where clause, we can see parameter list with values.
    The Properties window displays a Parameter List, the expanded view of which is shown in Figure 2-11, where we see a parameter named @1 and its corresponding compile time and runtime values.
    This is a very simple query, the optimizer has been able to perform a process called simple parameterization. 
    There is a process where the optimizer recognizes that, if you were using a parameter instead of the hard-coded value, it would be able to create an execution plan that can be reused. So, it substitutes a parameter of its own.
    In the below user query case, the optimizer parameterized our search argument so that the WHERE clause of our query is now WHERE d.GroupName = @1.
    As a result, we can see this parameter in the SELECT operator of our queries. 
    When you see this sort of parameterization, it is also important to inspect the query (in the SELECT operator) to check which of the hard-coded values in the original query is replaced by which parameter.
    
    If we change the predicate values to ‘sales’, there is change in parameter list values.
        For example
                use AdventureWorks2019
                go
                SELECT d.DepartmentID,d.Name,  d.GroupName
                FROM 
                HumanResources.Department AS d
                WHERE 
                d.GroupName = 'sales';
    
    Without simple parameterization, if we were to execute the query in with a different value in the search condition, such as WHERE d.GroupName = 'Sales and Marketing', then the query text has changed, no plan will match, and the optimizer will generate a new plan, even though we've executed what is essentially the same query.
    However, with our newly parameterized query and SQL Server swaps in the required value for the @1 parameter. Assuming no SET options change, the optimizer will reuse the existing plan. Figure 2-12 shows the Parameter List for a second execution of the query, with a different value supplied in the search condition.
    Whenever a parameter is used, the value passed to that parameter is used to compare to the statistics of the column or index being used. This is known as "parameter sniffing" (or "variable sniffing").


QueryHash and QueryPlanHash
—--------------------------------------------
    A QUERYHASH is a 
        - Number that is generated by reading the contents of a document or message.
        OR
        - Numeric value of a fixed length that uniquely identifies data.
        
        - This QueryHash value will be stored in Plan and will be used by Optimizer to identify plans with the same or very similar logic.
        - if the content/value of the user query matches with the HASH VALUE already in the plan Cache, Optimizer will REUSE the plan in the cache, assuming no difference in
        SET options, or database ID.
    
    A QUERYPLANHASH is a 
        - is the value for ENTIRE plan itself. Here, Optimizer will scans & identify different plans that are same in terms of the operations they perform, and the order they perform them.
    
    Different Cases & possibilities of QUERYHASH & QUERYPLANHASH.
        1) Same QueryHash and the same QueryPlanHash
            -- If we make a change only to literal values, and it doesn't affect the plan.
    
        2) Same QueryHash but different values for QueryPlanHash
            -- If we change only the literals but it results in a different plan

        3) Different QueryHash but the same QueryPlanHash
            -- If we make a logical change to the query that does not affect the execution plan


SET options
------------------
below parameter values can result in multiple plans for identical queries

Set Options					ANSI_N
------------				--------
ANSI_NULLS					True
ANSI_PADDING				True
ANSI_WARNINGS				True
ARITHABORT					True
CONCAT_NULL_YIELDS_NULL		True
NUMERIC_ROUNDABORT			False
QUOTED_IDENTIFIER			True

/*

Other Useful Tools and Techniques when Reading Plans
, it's often very useful to collect performance metrics alongside your execution plans,
especially when you're attempting to tune a query 

There are multiple ways to gather query metrics:
• SET STATISTICS IO/TIME
• Include Client Statistics
• SQL Trace (Profiler)
• Extended Events
• Query Store


*/
-- I/O and timing statistics using SET commands
use adventureworks2019
go

SET STATISTICS IO ON;
SET STATISTICS TIME ON;

 SELECT 
		 d.DepartmentID,
		 d.Name,
		 d.GroupName
 FROM HumanResources.Department AS d
 WHERE d.GroupName = 'Manufacturing';

SET STATISTICS IO OFF;
SET STATISTICS TIME OFF;


If you're attempting to tune a query and you want to see if it's running faster or
slower, as well as capture the number of reads, you need your measures to be accurate and
they simply won't be with STATISTICS IO

Also, it also doesn't always reveal all the work done. For example, if you have code that
makes a lot of calls to a user-defined function, it won't count that I/O, whereas Extended
Events does.

Include Client Statistics
-----------------------------
- If you are investigating queries that run fast but often, then the overhead of showing the
results in grid or text is often significant enough to invalidate the performance measurements
- Include Client Statistics option to look at the elapsed time(https://www.codeguru.com/database/sql-server-client-statistics/).

SQL Trace and Profiler
---------------------------
- The Profiler GUI uses a different buffering mechanism than Trace Events which can directly
affect your server in such a way that gathering metrics can negatively impact the server

- Trace Events can't be filtered at the point of capture. Instead, all Trace Events are captured and then filtered
afterwards, radically increasing their overhead on your system.

Extended Events
------------------
- capture your I/O and timing metrics using Extended Events. 
- They offer better and more effective filtering than Trace with lower impact on system.



/*
** -------------------------------- Chapter 3: Data Reading Operators:--------------------------------------**



We'll cover the following operators in detail:
• Clustered Index Scan
• Index Scan (nonclustered)
• Clustered Index Seek
• Index Seek (nonclustered)
• Key Lookup (clustered)
• Table Scan
• RID Lookup (heap).


Reading an Index
---------------------
- Traditional SQL Server indexes, consist of 8 K pages connected in a b+tree structure (balanced-tree, bushy-tree or even Bayer-tree).
- A clustered index is not a "copy" of the table. It is the table, with a b+tree structure built on top of it, so that the data is organized by the clustering
key. This explains why we can only create one clustered index per table.
- In addition to a clustered index, most tables have one or more nonclustered indexes, designed to improve the performance of of expensive queries. 
- A nonclustered index has the same b+tree structure, but the leaf-level pages do not contain the data rows, just the data for 
	- index key columns, 
	- clustered index key columns (assuming the table is not a heap), 
	- Any columns that we optionally add to the index using the INCLUDE clause.

- There are essentially three classes of operator that SQL Server can use to access data in
an index: 
> scan, 
> seek / lookup


*/

Index Scans 
-----------------
- In a scan operation, SQL Server navigates down to the first or last leaf-level page of the index and then scans forward or backward through the leaf pages.
- Sometimes, the optimizer prefers a scan over seek, when There is no usable index for the Predicate columns.
- Because the query is written in such a way that performing a seek against the index is not possible (for example, a function against a column will lead to scans).
- If a scan occurs 
	-- on a clustered index, we'll see the Clustered Index Scan operator
	-- on a nonclustered index, we'll see an Index Scan (nonclustered) operator.
	-- On a heap table, i.e. a table without a clustered index, you'll see a Table Scan

Clustered Index Scan
-----------------------------
For example : a simple query on the Employee table, looking for people with birthdays over 50 years ago.
	use adventureworks2019
	go
	SELECT 
	e.LoginID,
	e.JobTitle,
	e.BirthDate
	FROM HumanResources.Employee AS e
	WHERE e.BirthDate < DATEADD(YEAR, -50, GETUTCDATE());

some of the properties to look out for..
----------------------------------------------
- Estimated I/O Cost and Estimated CPU Cost: do not represent actual I/O and CPU measures.
- Even in an actual plan, these values represent the estimates from the optimizer based on statistics
- Estimated Number of Rows and the Actual Number of Rows : is the estimated and actual number of rows output by the operator. . 
  In this case, the operator outputs 26 rows ( out of 290, number of rows with a BirthDate more than 50 years in the past). 
- The Ordered property is False, as per the query, there was no order specified, else it would be True.
- Predicate applied by this operator = WHERE e.BirthDate < DATEADD(YEAR, -50, GETUTCDATE());
 i.e. each row of the table should undergo this function or predicate filter to match the condition of the query.
- An Index Scan is the same as a Clustered Index Scan. It's just against a different type of object. Let's examine the query

	use AdventureWorks2019
	go
	SELECT e.LoginID,
	e.BusinessEntityID
	FROM HumanResources.Employee AS e;
	- Since the query in question doesn't have a WHERE clause, there's little the optimizer can do to pick and choose how it's going to retrieve the information. It has to do a scan.

Are scans "bad?"
---------------------
No, Scans are not a "bad" thing. 
for example:
	- In our Clustered Index Scan example, the fact that the operator processes 290 rows to output only 21 won't have a significant impact on systems.
	- However, what if the optimizer opted to use a scan to output 21 rows from a table containing 3 million rows? At that point, we are performing a lot of unnecessary logical reads, 
	  and we may need to consider either tuning the query to make better use of our existing index, or adding an index
	- if there are any expressions or calculations on any indexed column, then the optimizer will have to calculate the value in each row to match the condition of the query, 
	  so the optimizer will have to use SCAN instead of SEEK.
	- when the statistics on an index become stale/outdated over time. In these cases, the optimizer can overestimate the number of rows, choosing to scan over a seek is efficient.
	- Sometimes,  a query may simply require all, or most, of the rows, so a scan is the most efficient way to do it.

Index seeks
-------------------
	
In a seek operation, SQL Server navigates directly to the page(s) containing the qualifying rows. a seek operator can sometimes become highly inefficient, 
for example, if inaccurate statistics have caused the optimizer to underestimate the massive number of rows, it will need the operator to process.

/*

		-- Clustered Index Seek
		Now that our query contains a search Predicate (BusinessEntityID) that matches the key of the clustered index,The seek operator uses the key values to identify the row, or rows.
		A seek operator has a property called Seek Predicates, which displays each of the predicates used to define the rows that need to be read:

		// Seek Keys[1]: Prefix: [AdventureWorks2014].[HumanResources].[Employee]. BusinessEntityID = Scalar Operator(CONVERT_IMPLICIT(int,[@1],0)) //

		we can see the effects of simple parameterization. we see a CONVERT_IMPLICIT function applied to the @1 parameter value, for BusinessEntityID, since the value we supplied (226) is inferred to be a smallint, and needs to be converted to an int to enable a seek.
		 If we passed a larger value, it would create the parameter as an int and it would create a second execution plan. 


*/

				use AdventureWorks2019
				go
				
				SELECT 
				e.BusinessEntityID,  e.NationalIDNumber,
				e.LoginID,  e.VacationHours, e.SickLeaveHours
				FROM 
				HumanResources.Employee AS e
				WHERE e.BusinessEntityID = 226;
				

Index Seek (nonclustered)
—----------------------------
Let's use a simple query against the Person.Person table.

-- Index Seek (nonclustered)
----------------------------

use AdventureWorks2019
go

SELECT 
		p.BusinessEntityID,
		 p.LastName,
		 p.FirstName
FROM Person.Person AS p
WHERE p.LastName LIKE 'Jaf%';

/*

Let's use a simple query against the Person.Person table. This query takes advantage of a nonclustered index (IX_Person_LastName_FirstName_MiddleName) on the table .
it's worth noting that for this Index Seek (nonclustered) operator, 
we see both 
> Predicate and 
> Seek Predicates properties. 

The Predicate looks like this, and essentially matches our WHERE clause:
	- [AdventureWorks2014].[Person].[Person].[LastName] as [p].[LastName] like N'Jaf%'

The Seek Predicates property shows the following:
	- Seek Keys[1]:  
	Start: [AdventureWorks2014].[Person].[Person].LastName >= Scalar Operator(N'Jaf'),
	End: [AdventureWorks2014].[Person].[Person].LastName < Scalar Operator(N'JaG')

Instead of a LIKE 'Jaf%', as was passed in the query, the optimizer has modified the logic it uses so that an additional filter has been added as follows
> Person.LastName >= 'Jaf' AND Person.LastName < 'JaG'

In this case, the optimizer optimized the WHERE clause Predicate, rewriting it from a LIKE condition to an interval defined by an AND condition. 
 Depending on collation, the predicates range changes from 'Jaf' -> 'JaG',  the interval might also contain values not matching the LIKE condition. Therefore, the latter is not removed but repeated in the Predicate property.
 In this example, all the columns required by the query are contained in the leaf level of the nonclustered index. In other words, this is a covering index for this query.


*/

Key lookups
—--------------
